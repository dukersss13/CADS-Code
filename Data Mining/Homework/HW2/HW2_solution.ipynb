{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 Multiple Linear Regression (4 points)\n",
    "a. Read in train.csv, remove all outliers and missing values (you can refer back to the solution of of HW1 posted on Canvas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "train = pd.read_csv('train.csv')\n",
    "# Get rid of the id columns that is irrelevant to the problem\n",
    "train = train.drop(columns=['Id'], axis=1)\n",
    "np.sum(train.isnull()) # Check to see if we have NAs in the data\n",
    "train = train.dropna() # Let's just get rid of the NAs.\n",
    "# Calculate the mean and standard deviation\n",
    "mu = np.mean(train['fixed acidity'])\n",
    "std = np.std(train['fixed acidity'])\n",
    "# Normalize the data\n",
    "normalized_data = (train['fixed acidity'] - mu)/std\n",
    "# Find the data index that is 3 standard deviation away from the distribution\n",
    "indexes = normalized_data < 3\n",
    "train_cleaned = train.loc[indexes, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Fit a single linear regression model on variable 'citric acid' using the Statsmodels package, print the summary of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   50.19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 May 2020</td> <th>  Prob (F-statistic):</th> <td>2.48e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:33:46</td>     <th>  Log-Likelihood:    </th> <td> -1303.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1104</td>      <th>  AIC:               </th> <td>   2610.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1102</td>      <th>  BIC:               </th> <td>   2620.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    5.3997</td> <td>    0.041</td> <td>  132.983</td> <td> 0.000</td> <td>    5.320</td> <td>    5.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8631</td> <td>    0.122</td> <td>    7.085</td> <td> 0.000</td> <td>    0.624</td> <td>    1.102</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 8.519</td> <th>  Durbin-Watson:     </th> <td>   1.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.014</td> <th>  Jarque-Bera (JB):  </th> <td>   8.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.199</td> <th>  Prob(JB):          </th> <td>  0.0145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.158</td> <th>  Cond. No.          </th> <td>    5.52</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.044\n",
       "Model:                            OLS   Adj. R-squared:                  0.043\n",
       "Method:                 Least Squares   F-statistic:                     50.19\n",
       "Date:                Mon, 04 May 2020   Prob (F-statistic):           2.48e-12\n",
       "Time:                        21:33:46   Log-Likelihood:                -1303.2\n",
       "No. Observations:                1104   AIC:                             2610.\n",
       "Df Residuals:                    1102   BIC:                             2620.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          5.3997      0.041    132.983      0.000       5.320       5.479\n",
       "x1             0.8631      0.122      7.085      0.000       0.624       1.102\n",
       "==============================================================================\n",
       "Omnibus:                        8.519   Durbin-Watson:                   1.914\n",
       "Prob(Omnibus):                  0.014   Jarque-Bera (JB):                8.469\n",
       "Skew:                           0.199   Prob(JB):                       0.0145\n",
       "Kurtosis:                       3.158   Cond. No.                         5.52\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code goes here\n",
    "selected = ['citric acid']\n",
    "X = train_cleaned[selected].values\n",
    "y = train_cleaned['quality'].values\n",
    "# Fit it using Statsmodel\n",
    "import statsmodels.api as sm\n",
    "constant_added = sm.add_constant(X)\n",
    "results = sm.OLS(y, constant_added).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Fit a single linear regression model on variable 'sulphates' using the Statsmodels package, print the summary of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   66.84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 May 2020</td> <th>  Prob (F-statistic):</th> <td>8.03e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:33:46</td>     <th>  Log-Likelihood:    </th> <td> -1295.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1104</td>      <th>  AIC:               </th> <td>   2595.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1102</td>      <th>  BIC:               </th> <td>   2605.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.8722</td> <td>    0.096</td> <td>   50.746</td> <td> 0.000</td> <td>    4.684</td> <td>    5.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.1585</td> <td>    0.142</td> <td>    8.176</td> <td> 0.000</td> <td>    0.880</td> <td>    1.437</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>10.415</td> <th>  Durbin-Watson:     </th> <td>   1.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.005</td> <th>  Jarque-Bera (JB):  </th> <td>  12.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.135</td> <th>  Prob(JB):          </th> <td> 0.00171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.452</td> <th>  Cond. No.          </th> <td>    8.66</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.057\n",
       "Model:                            OLS   Adj. R-squared:                  0.056\n",
       "Method:                 Least Squares   F-statistic:                     66.84\n",
       "Date:                Mon, 04 May 2020   Prob (F-statistic):           8.03e-16\n",
       "Time:                        21:33:46   Log-Likelihood:                -1295.3\n",
       "No. Observations:                1104   AIC:                             2595.\n",
       "Df Residuals:                    1102   BIC:                             2605.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.8722      0.096     50.746      0.000       4.684       5.061\n",
       "x1             1.1585      0.142      8.176      0.000       0.880       1.437\n",
       "==============================================================================\n",
       "Omnibus:                       10.415   Durbin-Watson:                   1.921\n",
       "Prob(Omnibus):                  0.005   Jarque-Bera (JB):               12.742\n",
       "Skew:                           0.135   Prob(JB):                      0.00171\n",
       "Kurtosis:                       3.452   Cond. No.                         8.66\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code goes here\n",
    "selected = ['sulphates']\n",
    "X = train_cleaned[selected].values\n",
    "y = train_cleaned['quality'].values\n",
    "# Fit it using Statsmodel\n",
    "import statsmodels.api as sm\n",
    "constant_added = sm.add_constant(X)\n",
    "results = sm.OLS(y, constant_added).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Fit a multiple linear regression model on 'citric acid' and 'sulphates' together using the Statsmodels package, print the summary of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   46.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 04 May 2020</td> <th>  Prob (F-statistic):</th> <td>5.72e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:33:46</td>     <th>  Log-Likelihood:    </th> <td> -1283.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1104</td>      <th>  AIC:               </th> <td>   2573.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1101</td>      <th>  BIC:               </th> <td>   2588.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.8516</td> <td>    0.095</td> <td>   51.007</td> <td> 0.000</td> <td>    4.665</td> <td>    5.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.6170</td> <td>    0.126</td> <td>    4.903</td> <td> 0.000</td> <td>    0.370</td> <td>    0.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.9358</td> <td>    0.147</td> <td>    6.348</td> <td> 0.000</td> <td>    0.647</td> <td>    1.225</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.554</td> <th>  Durbin-Watson:     </th> <td>   1.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.023</td> <th>  Jarque-Bera (JB):  </th> <td>   8.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.105</td> <th>  Prob(JB):          </th> <td>  0.0115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.388</td> <th>  Cond. No.          </th> <td>    9.30</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.077\n",
       "Model:                            OLS   Adj. R-squared:                  0.076\n",
       "Method:                 Least Squares   F-statistic:                     46.14\n",
       "Date:                Mon, 04 May 2020   Prob (F-statistic):           5.72e-20\n",
       "Time:                        21:33:46   Log-Likelihood:                -1283.4\n",
       "No. Observations:                1104   AIC:                             2573.\n",
       "Df Residuals:                    1101   BIC:                             2588.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.8516      0.095     51.007      0.000       4.665       5.038\n",
       "x1             0.6170      0.126      4.903      0.000       0.370       0.864\n",
       "x2             0.9358      0.147      6.348      0.000       0.647       1.225\n",
       "==============================================================================\n",
       "Omnibus:                        7.554   Durbin-Watson:                   1.928\n",
       "Prob(Omnibus):                  0.023   Jarque-Bera (JB):                8.936\n",
       "Skew:                           0.105   Prob(JB):                       0.0115\n",
       "Kurtosis:                       3.388   Cond. No.                         9.30\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code goes here\n",
    "selected = ['citric acid', 'sulphates']\n",
    "X = train_cleaned[selected].values\n",
    "y = train_cleaned['quality'].values\n",
    "# Fit it using Statsmodel\n",
    "constant_added = sm.add_constant(X)\n",
    "results = sm.OLS(y, constant_added).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Compare the goodness of fit of the three models you have from b, c, and d. Which one do you think is a better fit for the data you have? Explain your answer using the R<sup>2</sup>, adj-R<sup>2</sup>, AIC, and BIC, if necessarry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model fitted with two variables together give a better fit\n",
    "gives a better fit to the data. The R squared and adj-R squared is higher compare\n",
    "to the model fitted separately.\n",
    "BIC and AIC is also showing the same result because they are lower\n",
    "for the mulitple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1 (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Implement a function that takes in the *train.csv* dataset and returns a cleaned dataset without outliers and NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "def preprocessing_pipeline(train):\n",
    "    ## Your code goes here\n",
    "    # Get rid of the id columns that is irrelevant to the problem\n",
    "    train = train.drop(columns=['Id'], axis=1)\n",
    "    np.sum(train.isnull()) # Check to see if we have NAs in the data\n",
    "    train = train.dropna() # Let's just get rid of the NAs.\n",
    "    # Calculate the mean and standard deviation\n",
    "    mu = np.mean(train['fixed acidity'])\n",
    "    std = np.std(train['fixed acidity'])\n",
    "    # Normalize the data\n",
    "    normalized_data = (train['fixed acidity'] - mu)/std\n",
    "    # Find the data index that is 3 standard deviation away from the distribution\n",
    "    indexes = normalized_data < 3\n",
    "    processed_train = train.loc[indexes, :]\n",
    "    return processed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Implement a function that takes in the R<sup>2</sup> of a regression and return the VIF value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(r_squared):\n",
    "    ## Your code goes here\n",
    "    vif = 1/(1-r_squared)\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Implement a function that takes in preprocessed data and returns a dataframe that contains the VIF for each variable. The dataframe should contain two columns: variable names and their VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vif_dataframe(processed_train):\n",
    "    ## Your code goes here\n",
    "    # get the data without quality column\n",
    "    vif_train = processed_train.drop(['quality'], axis=1)\n",
    "    columns = vif_train.columns\n",
    "    vifs = []\n",
    "    for col in columns:\n",
    "        X = vif_train.drop(col, axis=1).values\n",
    "        y = vif_train[col].values\n",
    "        # Fit it using Statsmodel\n",
    "        constant_added = sm.add_constant(X)\n",
    "        results = sm.OLS(y, constant_added).fit()\n",
    "        results.summary()\n",
    "        vif = calculate_vif(results.rsquared)\n",
    "        vifs.append(vif)\n",
    "    vif_dataframe = pd.DataFrame({'vifs': vifs, 'cols': columns})\n",
    "    return vif_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Use the functions that you developed from Q1 and generate a VIF dataframe for all the variables in the wine dataset. Discuss the VIF values you find and the effect of multicollinearity on regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "processed_train = preprocessing_pipeline(train)\n",
    "vif_dataframe = generate_vif_dataframe(processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vifs</th>\n",
       "      <th>cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.718711</td>\n",
       "      <td>fixed acidity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.782956</td>\n",
       "      <td>volatile acidity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.159279</td>\n",
       "      <td>citric acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.702938</td>\n",
       "      <td>residual sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.513004</td>\n",
       "      <td>chlorides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.971999</td>\n",
       "      <td>free sulfur dioxide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.182210</td>\n",
       "      <td>total sulfur dioxide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.147005</td>\n",
       "      <td>density</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.388611</td>\n",
       "      <td>pH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.415035</td>\n",
       "      <td>sulphates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.022363</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        vifs                  cols\n",
       "0   7.718711         fixed acidity\n",
       "1   1.782956      volatile acidity\n",
       "2   3.159279           citric acid\n",
       "3   1.702938        residual sugar\n",
       "4   1.513004             chlorides\n",
       "5   1.971999   free sulfur dioxide\n",
       "6   2.182210  total sulfur dioxide\n",
       "7   6.147005               density\n",
       "8   3.388611                    pH\n",
       "9   1.415035             sulphates\n",
       "10  3.022363               alcohol"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vif_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe we can see that the variable 'fixed acidity' and 'density' have high vif values. High vif values indicates that these two variables are highly correlated with some variables in the dataset - multicollinearity.\n",
    "\n",
    "The effect of mnulticollinearity causes the regression model to be unstable. High multicollinearity violate the assumption that all the variables are independent of each other which makes all statistics (p-values, etc) unreliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Use the VIF dataframe you generated from Q1 and choose a subset of variables that does not have high multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "columns_to_drop = ['fixed acidity']\n",
    "subset = processed_train.drop(columns_to_drop, axis=1)\n",
    "vifs = generate_vif_dataframe(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vifs</th>\n",
       "      <th>cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.777803</td>\n",
       "      <td>volatile acidity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.827359</td>\n",
       "      <td>citric acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.427160</td>\n",
       "      <td>residual sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.434793</td>\n",
       "      <td>chlorides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.957240</td>\n",
       "      <td>free sulfur dioxide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.088190</td>\n",
       "      <td>total sulfur dioxide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.356526</td>\n",
       "      <td>density</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.628829</td>\n",
       "      <td>pH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.386075</td>\n",
       "      <td>sulphates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.105293</td>\n",
       "      <td>alcohol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vifs                  cols\n",
       "0  1.777803      volatile acidity\n",
       "1  2.827359           citric acid\n",
       "2  1.427160        residual sugar\n",
       "3  1.434793             chlorides\n",
       "4  1.957240   free sulfur dioxide\n",
       "5  2.088190  total sulfur dioxide\n",
       "6  2.356526               density\n",
       "7  1.628829                    pH\n",
       "8  1.386075             sulphates\n",
       "9  2.105293               alcohol"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vifs # Without fixed acidity we have all variables with low vifs (below 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Fit a multiple linear regression model and print a summary of the regression result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "selected = vifs['cols']\n",
    "X = train_cleaned[selected].values\n",
    "y = train_cleaned['quality'].values\n",
    "# Fit it using Statsmodel\n",
    "constant_added = sm.add_constant(X)\n",
    "results = sm.OLS(y, constant_added).fit()\n",
    "#results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use 5-fold cross validation to validate the mean squared error (MSE) of a multiple linear regression model fit to the subset of data. (HINT: You will need to implement a function to calculate the MSE. See this link for more details: https://en.wikipedia.org/wiki/Mean_squared_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "def calculate_mse(y, y_pred):\n",
    "    # Good practice to check if y and y_pred have the same leng\n",
    "    assert len(y) == len(y_pred) \n",
    "    return np.sum(np.square(y_pred - y))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40174685845566843,\n",
       " 0.40283538074422953,\n",
       " 0.46849866801732726,\n",
       " 0.413071255807118,\n",
       " 0.39985820926509974]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "mses = []\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    constant_added = sm.add_constant(X[train,:])\n",
    "    results = sm.OLS(y[train], constant_added).fit()\n",
    "    y_pred = results.predict(sm.add_constant(X[test,:]))\n",
    "    mses.append(calculate_mse(y[test], y_pred))\n",
    "\n",
    "mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Make a prediction on the test set using the model you fit in 2b and submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "selected = vifs['cols']\n",
    "X = train_cleaned[selected].values\n",
    "y = train_cleaned['quality'].values\n",
    "# Fit it using Statsmodel\n",
    "constant_added = sm.add_constant(X)\n",
    "results = sm.OLS(y, constant_added).fit()\n",
    "\n",
    "# test set\n",
    "test = pd.read_csv('test.csv')\n",
    "X_test = test[selected].values\n",
    "y_pred = results.predict(sm.add_constant(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'sample_submission.csv' does not exist: b'sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5daa047d76df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Predicted'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;31m# Change the Predicted column to your prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'second_submission.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'sample_submission.csv' does not exist: b'sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission.loc[:, 'Predicted'] = y_pred # Change the Predicted column to your prediction\n",
    "sample_submission.to_csv('second_submission.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Compare the Kaggle score and the cross-validation result. Is the cross-validation result a good representation of the prediction accuracy on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your discussion here\n",
    "np.mean(mses) # Kaggle score 0.461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Discuss the difference between goodness of fit on the training set, on the validation set, and on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goodness of fit on the training set is how well your data fit the training data, however, it doesn't tell you have good your model in predicting unseen data. You might over-fit your training set, and you model gives bad predictions. You can theoretically have a model that fits the data perfectly using linear regression using the same number of variables to the number of records you have.\n",
    "\n",
    "The validation accuracy is a good estimate how your data predicts on unseen data using the training set you have. It is usually used to do parameter selection. You want a model to give you as good as possible in predicting test set, however, you cannot fit any model/parameter using the test set, or it will likely to also fit to the test set as well. You can only do model fitting using your training set, the test set should be left untouched until you have a valid model and want to see if your prediction make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
