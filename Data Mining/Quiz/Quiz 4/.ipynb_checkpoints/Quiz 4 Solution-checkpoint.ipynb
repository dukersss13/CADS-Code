{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 530 Data Mining Quiz 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (4 Points) (Work on this question on a piece of paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a dataset with 6 records:\n",
    "\n",
    "(19, 19), (30, 15), (25, 25), (16, 15), (19, 12), (28, 20)\n",
    "\n",
    "a. Use single-linkage agglomerative hierarchical clustering on the dataset to draw the resulting dendrogram. You are welcome to consult the course slides and your notes on your computer. Otherwise, you can use your computer only to program a Python function to calculate the Euclidean distance between data points.\n",
    "\n",
    "b. Pick the cutoff value that you think gives the best clustering results. Explain your choice.\n",
    "\n",
    "c. There are claims that hierarchical methods impose hierarchical structure on the data whether or not such structure exists in the data. Do you agree with this claim? Put differently, if you run hierarchical clustering on a dataset and want K clusters, can you just assume that you can cut it to get those K clusters, or are there some things you need to look for in the dendrogram before simply cutting? If there are such things, what are they?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(19, 19), (30, 15), (25, 25), (16, 15), (19, 12), (28, 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.041594578792296"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.euclidean(X[4], X[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Because we have the largest distances between the two clusters while we choose 6 as the cut-off value.\n",
    "\n",
    "c. Answer: No, because even though you can create a hierarchical structure from the data, it doesn't mean that the data have an underlying structure. For example, you can run hierarchical clustering in a gaussian distributed dataset which does not carry any underlying clustering structure, however, you can still get a hierarchy from it and choose a cut off value to impose a structure. You have to be careful while you are interpreting the hierarchy. Good clusters have large distances from each other, which usually means that they are higher up and have a large distances from the previous linkage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (6 points)\n",
    "\n",
    "a.Implement a function that performs PCA analysis. The input to the function is the dataset, X (# samples x # features), and the number of features that you would like to keep. The function should return a matrix with the principal components, and a vector of the percentage of variances contains by each principal components. (Hint: You should use the [numpy package](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) to get the eigen vectors and eigen values of a matrix. Also see [numpy matrix multiplication](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html). Obviously, you cannot use scikit-learn’s PCA function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_percentage_of_variance(eigen_values, components):\n",
    "    # Your code goes here\n",
    "    list_of_percentages = np.abs(eigen_values)/np.sum(np.abs(eigen_values))\n",
    "    return list_of_percentages[:2]\n",
    "\n",
    "def pca(X, components=2):\n",
    "    # Your code goes here\n",
    "    C = np.matmul(X.T, X)\n",
    "    w, v = np.linalg.eig(C)\n",
    "    Z = np.matmul(X, v)\n",
    "    pcs = Z[:,:components]\n",
    "    \n",
    "    list_of_percentages = caculate_percentage_of_variance(w, components)\n",
    "    return pcs, list_of_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use your function to run PCA analysis on the dataset below, keep the first two principal components, and see how much variance you preserve with the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs, ls = pca(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38340578, -0.2935787 ],\n",
       "       [-2.22189802,  0.25133484],\n",
       "       [-3.6053038 , -0.04224385],\n",
       "       [ 1.38340578,  0.2935787 ],\n",
       "       [ 2.22189802, -0.25133484],\n",
       "       [ 3.6053038 ,  0.04224385]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99244289, 0.00755711])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls # 99% of variance preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.38340578,  0.2935787 ],\n",
       "        [ 2.22189802, -0.25133484],\n",
       "        [ 3.6053038 ,  0.04224385],\n",
       "        [-1.38340578, -0.2935787 ],\n",
       "        [-2.22189802,  0.25133484],\n",
       "        [-3.6053038 , -0.04224385]]), array([0.99244289, 0.00755711]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_sklearn = PCA(2).fit(X)\n",
    "pca_sklearn.transform(X), pca_sklearn.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. (1 point extract credit) You have a cloud of points for which $y ⃗=β_0+β_1 x ⃗+ε ⃗$   for $y ⃗,x ⃗,ε ⃗∈R^n$ (see figure below). We learned in class how to run a regression line through the data. We also now learned that, if we concatenate $x ⃗$ and $y ⃗$, we could treat it as a dataset, run PCA through it, and find the first principal direction (i.e., the loadings of the first principal component) for the dataset. Ignoring the fact that the dataset is not centered at the origin, would you expect the first principal direction to be aligned with the regression line? Explain your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are not the same. You are minimizing the residual sum of squares (RSS) when you are fitting a linear regression, while PCA is looking for a direction that maximize the variance of data points prejected onto it. Also, the RSS is minimized in the y direction only, whle the PCA projects the data perpendicularly onto the principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
