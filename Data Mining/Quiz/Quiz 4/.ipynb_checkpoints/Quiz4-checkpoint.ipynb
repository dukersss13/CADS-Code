{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 530 Data Mining Quiz 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (4 Points) (Work on this question on a piece of paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a dataset with 6 records:\n",
    "\n",
    "(19, 19), (30, 15), (25, 25), (16, 15), (19, 12), (28, 20)\n",
    "\n",
    "a. Use single-linkage agglomerative hierarchical clustering on the dataset to draw the resulting dendrogram. You are welcome to consult the course slides and your notes on your computer. Otherwise, you can use your computer only to program a Python function to calculate the Euclidean distance between data points.\n",
    "\n",
    "b. Pick the cutoff value that you think gives the best clustering results. Explain your choice.\n",
    "\n",
    "c. There are claims that hierarchical methods impose hierarchical structure on the data whether or not such structure exists in the data. Do you agree with this claim? Put differently, if you run hierarchical clustering on a dataset and want K clusters, can you just assume that you can cut it to get those K clusters, or are there some things you need to look for in the dendrogram before simply cutting? If there are such things, what are they?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (6 points)\n",
    "\n",
    "a.Implement a function that performs PCA analysis. The input to the function is the dataset, X (# samples x # features), and the number of features that you would like to keep. The function should return a matrix with the principal components, and a vector of the percentage of variances contains by each principal components. (Hint: You should use the [numpy package](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) to get the eigen vectors and eigen values of a matrix. Also see [numpy matrix multiplication](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html). Obviously, you cannot use scikit-learn’s PCA function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def caculate_percentage_of_variance(eigen_values, components):\n",
    "    # Your code goes here\n",
    "    i = 0 \n",
    "    eigSort = np.sort(eigen_values)\n",
    "    list_of_percentages = []\n",
    "    n = len(eigSort)\n",
    "    while i < n:\n",
    "        percentages = eigSort[-i]/(np.sum(eigSort))\n",
    "        list_of_percentages.append(percentages)\n",
    "        i += 1\n",
    "    return list_of_percentages\n",
    "\n",
    "def pca(X, components=2):\n",
    "    # Your code goes here\n",
    "    \n",
    "    eigen_values =  np.linalg.eigvals(X)\n",
    "    eigen = np.linalg.eig(X)\n",
    "    list_of_percentages = calculate_percentage_of_variance(eigen_values, components)\n",
    "    eigSort = np.sort(eigen_values)\n",
    "    \n",
    "    \n",
    "    return pcs, list_of_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use your function to run PCA analysis on the dataset below, keep the first two principal components, and see how much variance you preserve with the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. (1 point extract credit) You have a cloud of points for which $y ⃗=β_0+β_1 x ⃗+ε ⃗$   for $y ⃗,x ⃗,ε ⃗∈R^n$ (see figure below). We learned in class how to run a regression line through the data. We also now learned that, if we concatenate $x ⃗$ and $y ⃗$, we could treat it as a dataset, run PCA through it, and find the first principal direction (i.e., the loadings of the first principal component) for the dataset. Ignoring the fact that the dataset is not centered at the origin, would you expect the first principal direction to be aligned with the regression line? Explain your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](picture1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
